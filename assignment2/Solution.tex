%%This is a very basic article template.
%%There is just one section and two subsections.
\documentclass[a4paper, 11pt]{article}
\usepackage[pdftex]{graphicx}
\usepackage{parskip}
\usepackage{hyperref}
\usepackage[all]{hypcap}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{dsfont}
\title{Assignment 2 \\ Math 205 Linear Algebra}

\author{
  Muhammad Shahrom Ali\\ ma03559
  \and
  Syed Ammar Mahdi\\  sm03691
}

\newcommand{\mat}[1]{\boldsymbol { \mathsf{#1}} }

\begin{document}
\setlength{\parskip}{10pt} % 1ex plus 0.5ex minus 0.2ex}
\setlength{\parindent}{0pt}
\DeclareGraphicsExtensions{.pdf,.png,.gif,.jpg}
\maketitle




\section{Answers}
%Remove the question statements when question complete


\begin{enumerate} 

\item Linear Independence is a concept applicable to a set of vectors. What it says is that in a set of vectors, if none of the vectors can be expressed as a linear combination of the other vectors (excluding itself, of course), then the set of vectors is linearly independent. It can be mathematically written as 

\begin{center}
$\forall \vec v_i \in \{ \vec v_1, \vec v_2, \vec v_3, \dots, \vec v_n \}, \vec v_i \neq a_1 \vec v_1 + a_2 \vec v_2 + \dots + a_{n-1} \vec v_{n-1}$
\end{center}

What it means is that each of the vectors in this set of linearly independent vectors, is contributing to the span of the set. i.e. if a vector was removed, the span of the whole set would be reduced. The set is called `linearly' independent. Intuitively speaking Linear implying in a line, and independence from that would mean no two vectors lie on the same line. But since the concept is general to $n$ dimensions, it doesnt just talk about lying on the same line. Since any vectors that are colinear can be represented as a scalar multiple of the other, we can generalize that as being a linear combination. And hence we can conclude that a set of vectors is linearly dependent if atleast one vector in the set can be represented as a linear combination of the other vectors.

If we negate the above mathematical statement it looks something like:

\begin{center}
$\exists \vec v_i \in \{ \vec v_1, \vec v_2, \vec v_3, \dots, \vec v_n \}, \vec v_i = a_1 \vec v_1 + a_2 \vec v_2 + \dots + a_{n-1} \vec v_{n-1}$

$a_1 \vec v_1 + a_2 \vec v_2 + \dots + a_{n-1} \vec v_{n-1} - \vec v_i = 0$
\end{center} 

So this brings us to the formal definition of linear dependence of a set. If the linear combination of the vectors in the set is equal to zero apart from the trivial solution (all scalars = 0), then the set of vectors is linearly dependent. A set containing the zero vector is always linearly dependent. If a subset of a set of vectors is linearly dependent, then the set is also linearly dependent. \cite{LinInd}

The Basis of a vector space $\textbf{A}$ is \textbf{the linearly independent set of vectors} such that, the vectors in the set span the whole of $\textbf{A}$. Since it is linearly independent, it is the least number of vectors required to span the space and removal of any of the vectors will reduce the span of the vectors. For any $\mathds{R}^n$ the bases may be infinitely many but the number of vectors in each of them remains the same. And the number of vectors in the Basis is known as the dimension of the vector space. The dimension of a basis of a vector space in $\mathds{R}^n$ is $n$. \cite{dimBases}

\begin{enumerate}[label=\alph*)]
\item 
We form a matrix of these vectors: 

$
\begin{bmatrix}
1 & 3 & 1 & -14 \\
4 & 0 & 1 & 13 \\
5 & -1 & - 2 & 7 \\
2 & 4 & 1 & -19
\end{bmatrix}
$

Perform following row operations: 

\begin{center}
$-4R_1 + R_2 $

$-5R_1 + R_3$

$-2R_1 + R_4$

$\frac{1}{3} R_2$

$-4R_2 + R_3$

$-R_2 + 2R_4$

$-\frac{1}{3}R_3 + R_4$
\end{center}

We get the matrix:

$
\begin{bmatrix}
1 & 3 & 1 & -14 \\
0 & -4 & -1 & 23 \\
0 & 0 & - 3 & -15 \\
0 & 0 & 0 & 0
\end{bmatrix}
$ 

The last column can be written as a linear combination of the other 3 vectors, it does not have a pivot, and removing it doesn't impact the span of the vectors. Hence, these are lineally dependent.


\item No, because there will be a vector in the matrix such that it can be recreated by a linear combination. In a subspace of $\mathds{R}^m$ there can be at most $m$ independent vectors and since $n > m$, the columns wont be linearly independent. 

\item Yes. In a subspace of $\mathds{R}^m$ there can be \textbf{at most} $m$ linearly independent vectors. A subset of a linearly independent set of vectors is also linearly independent. And since $n < m$, the columns of the tall matrix can be linearly independent.

\item  Only the trivial solution, Where $\vec x$ is the zero vector.

\item  We will see non-zero diagonal elements in $\mat A$.

\item No. We require a set of $m$ vectors which are linearly independent as well, and hence allow to span the whole of the subspace in $\mathds{R}^m$.


\end{enumerate}

\item $\mat A = \begin{bmatrix} 
    3 & 6 & 9 \\ 
    6 & 9 & 12 \\
    9 & 12 & 15 \end{bmatrix}$
    
     \begin{enumerate}[(a)]
         \item To find $N(\mat A)$, we must reduce $\mat{A}$ to reduced row echelon form (\textbf{RREF}). The following row operations are performed:
         
         	\begin{center}
	$-2R_1 + R_2 $

	$-\frac{1}{3}R_2$

	$-3R1 + R3$

	$6R_2 + R3$

	$-6R2 + R1$

	$\frac{1}{3}R_1$

	\end{center}

	We get the following matrix in \textbf{RREF}:
	
	\begin{center}
	$\mat A = \begin{bmatrix} 
	    1 & 0 & -1 \\ 
	    0 & 1 & 2 \\
	    0 & 0 & 0 \\
	    \end{bmatrix}$
	\end{center}

	
	From the above matrix, we can see that we have one free variable, $x_3$. The rank of $\mat{A}$ is 2. This means that the dimension of the null space (the nullity) of our vector space will be 1, as the \textit{rank-nullity theorem} implies that:
	
	\begin{equation}
		rank(\mat{A}) + nullity(\mat{A}) = n
	\end{equation}

	Where $n$ is the dimension of the matrix (the number of columns), $rank(\mat{A})$ is the number of $pivots$ in the matrix, and $nullity(\mat{A})$ is the dimension of $N(\mat A)$ (or the number of vectors that span the column).
	
	We thus have the following sets of equations for the free variable $x_3$:
	
	\begin{center}
		$x_1 = -x_3$ \\
		$x_2 = -2x_3$ \\
		$x_3 = x_3$ \\
	\end{center}
	
	We thus rewrite the above equations in vector form:
	
	\begin{equation*}
		\vec{x} = x_3
		\begin{bmatrix}
			1 \\
			-2 \\
			1 \\
		\end{bmatrix}
	\end{equation*}
	
	Thus vector $\vec{x}$ will be the vector that spans the null space. Intuitively, the solution looks like a point in $\mathds{R}^3$.
      
	      \item $\mat B = \begin{bmatrix} 1 & -1 \\ -1 & 1 \end{bmatrix}$
	      
	      To solve for $N(\mat{B})$, we must solve for $\mat{B}\textbf{x} = \vec_{0}$.
	      
	      Here, $\mat{B}$ is already in \textbf{RREF}. We can thus use the following relation implied from the \textit{rank-nullity theorem} to intuitively find out what the null-space should look like:
	      
	      \begin{equation*}
	      	rank(\mat{B}) = dim(\mat{B}) \iff nullity(B) = 0
	      \end{equation*}
	      
	     This means that this equation has only the trivial solution in $N(\mat{B})$, which is the $\vec{0}$ vector.
	     
	     Similarly, to solve for $N(\mat{B}^2)$, we must solve for $\mat{B}^2 \textbf{x} = \vec_{0}$.
	     
	     First, we compute $\mat{B}^2$:

	    \begin{center}
	        $$\mat{B}^2 = 
	        \begin{bmatrix}
	        1 & -1 \\
	        -1 & 1 \\
	        \end{bmatrix}
	        \begin{bmatrix}
	        1 & -1 \\
	        -1 & 1 \\
	        \end{bmatrix}
	        =
	        \begin{bmatrix}
	        2 & -2 \\
	        -2 & 2 \\
	        \end{bmatrix}$$
	    \end{center}
	    
	    By re-writing $\mat{B}^2$ in \textbf{RREF}, we can see that it is row equivalent to $\mat{B}$. That is, the row operations $\frac{1}{2}R_1$ and $\frac{1}{2}R_2$ reduce $\mat{B}^2$ to $\mat{B}$.
	    
	    Thus, as the two matrices are row-equivalent and have $rank = 0$:
	    
	    \begin{equation*}
	        N(\mat{C}) = N(\mat C^2) = \emptyset
	    \end{equation*}
	     
	      \item Assume for this part that $\mat C$ can be any arbitrary square matrix
	    
	    Suppose we have an arbitrary $m \times n$ matrix $\mat{C}$ such that:
	    
	    \begin{equation*}
	        \mat{C} =
	        \begin{bmatrix}
	        c_{1,1} & c_{1,2} & \cdots & c_{1,n}\\
	        c_{2,1} & c_{2,2} & \cdots & c_{2,n} \\
	        \vdots & \vdots & \ddots & \vdots \\
	        c_{m,1} & c_{m,2} & \cdots & c_{m,n} \\
	        \end{bmatrix}
	    \end{equation*}
	    
	    \begin{equation*}
	        \mat{C}^2 =
	        \begin{bmatrix}
	        0 & 0 & \cdots & 0 \\
	        0 & 0 & \cdots & 0 \\
	        \vdots & \vdots & \ddots & \vdots \\
	        0 & 0 & \cdots & 0 \\
	        \end{bmatrix}
	    \end{equation*}
	    
	    That is, the matrix $\mat{C}$ with dimensions $m \times n$ such that $\mat{C}^2 = \mat{0}_{m,n}$, the zero matrix with dimensions $m \times n$, and $\mat{C} \neq \mat{0}_{m,n}$.
	    
	    If such a matrix exists, then from \textit{theorem (1)} we can deduce that
	    \begin{center}
	     $nullity(\mat{C}^2) = n$ \\
	     $nullity(\mat{C}) < n$ \\
	    \end{center}
	   
	   In other words, $N(\mat{C})$ would have non-trivial solutions, but $N(\mat{C}^2)$ would span all of $\mathbb{R}^n$. Note here that $N(\mat{C}) \neq N(\mat{C}^2)$, as the only way $N(\mat{C})$ can span all of $\mathbb{R}^n$ is if $\mat{C}$ has $rank = 0$ (no pivot columns), which is only possible when $\mat{C}$ is the zero matrix, and we have already stated cannot be the case.
	    
	    So such matrices $\mat{C}$ would have at least $rank = 1$ (one pivot column), and thus we have:
	    
	    \begin{equation*}
	        nullity(\mat{C}) < nullity(\mat{C}^2)
	    \end{equation*}
	    
	    If we can provide one example of such a matrix that exists, we can prove that not all matrices have $nullity(\mat{C}) = nullity(\mat{C}^2)$.
	    
	    An example of one such matrix is the matrix and it's square:
	    
	    \begin{equation*}
	        \mat{D} = 
	        \begin{bmatrix}
	            0 & 0 \\
	            0 & 1 \\
	        \end{bmatrix}
	    \end{equation*}
	 
	    \begin{equation*}
	        \mat{D}^2 = 
	        \begin{bmatrix}
	            0 & 0 \\
	            0 & 0 \\
	        \end{bmatrix}
	    \end{equation*}
	    
	    Thus, we conclude that unless $\mat{C}^2$ is row equivalent to $\mat{C}$,
	    
	    \begin{equation*}
            N(\mat{C}) \neq N(\mat{C}^2)
	    \end{equation*}
	    
	    \end{enumerate}
    
\item A linear system $\mat A \vec x = \vec b$ has special solution of the form
\begin{align}
 \vec x_n = x_2 \begin{bmatrix} 1 \\ 1 \\ 0 \\ 0 \end{bmatrix}
 \end{align} 
\begin{enumerate}[label=(\alph*)]
\item What are the basis for $N(\mat A)$
\item What is the dimension of $N(\mat A)$
\item Write the reduced row echelon form for $\mat A$
\item Describe intuitively the geometry of the solution space
\end{enumerate}

\item Determine with reason which of the following are sub spaces of $3 \times 3$ matrix $\mat M$
\begin{enumerate}[label=(\alph*)]
\item all $3 \times 3$ matrices $\mat A$ such that $\mat A^T = -\mat A$
\item all $3 \times 3$ matrices such that the linear system $\mat A \vec x = \vec 0$ has only trivial solution
\end{enumerate}



\item For which right sides are these systems solvable? Give your reasons

For all $\vec b$ in the $C(\mat A)$, there exists a solution for the system

\begin{enumerate}
\item
\[ 
\left[ \begin{array}{ccc}
5  &   7 &  5\\
10  &   14 &  15\\
20 & 28 & 25
\end{array} \right]
%
\left[ \begin{array}{c}
 x_1\\
 x_2\\
x_3 
\end{array} \right]
%
= \left[ \begin{array}{c}
 b_1\\
 b_2\\
 b_3 
\end{array} \right]
\]

Applying row operations on $\mat A$: 

\begin{center}
$-2R_1 + R_2$

$-4R_1 + R_3$

$-R_3 + R_1$

$-R_3 + R_2$

$\frac{1}{5} R_1$

$\frac{1}{5} R_3$

$ R_2 \leftrightarrow R_3$
\end{center}


\[ 
\left[ \begin{array}{ccc}
1  & \frac{7}{5} &  0\\
0  &   0 &  5\\
0 & 0 & 0
\end{array} \right]
\]

$\vec a_1$ and $\vec a_3$ are linearly independent and their span is the $C(\mat A)$ hence 

$\forall \vec b \in C(\mat A)$  the system is solvable.


\item
\[ 
\left[ \begin{array}{cc}
1  &   4 \\
2  &  9 \\
-1 & -4 
\end{array} \right]
%
\left[ \begin{array}{c}
 x_1\\
 x_2\\
\end{array} \right]
%
= \left[ \begin{array}{c}
 b_1\\
 b_2\\
 b_3 
\end{array} \right]
\]

Applying row operations on $\mat A$

\begin{center}
	
$R_1 + R_3$

$-2R_1 + R_2$

$-4R_2 + R_1$

$
\begin{bmatrix}
1 & 0 \\ 
0 & 1 \\
0 & 0
\end{bmatrix}
$	
\end{center}

$\forall \vec b$ where $\vec b$ is the linear combination of the two columns of  $\mat A$, the system is solvable.

So speaking in terms of the Cartesian coordinate system, all vectors lying on the $xy$ plane, with $z$ component zero.

\item 
\[ 
\left[ \begin{array}{ccc}
1  & 1 & 1\\
0  & 1 & 1\\
0 & 0 & 1
\end{array} \right]
%
\left[ \begin{array}{c}
 x_1\\
 x_2\\
x_3
\end{array} \right]
%
= \left[ \begin{array}{c}
 b_1\\
 b_2\\
 b_3 
\end{array} \right]
\]

Since all three vectors of this $3\times3$ matrix are independent, the system is solvable for all vectors $\vec b$ in $\mathds{R}^3$.
\item
\[
 \left[ \begin{array}{ccc}
1  & 1 & 1\\
0  & 1 & 1\\
0 & 0 & 0
\end{array} \right]
%
\left[ \begin{array}{c}
 x_1\\
 x_2\\
x_3
\end{array} \right]
%
= \left[ \begin{array}{c}
 b_1\\
 b_2\\
 b_3 
\end{array} \right]
\]

In this part, $\mat A$ has only 2 independent vectors, hence similar to part b, the system is solvable for all vectors $\vec b$ which are the linear combinations of $\vec a_1$ and $\vec a_2$.

\end{enumerate}

\item Given that $\mat A$ is an arbitrary $4 \times 3$ matrix, if we add an extra column $\vec a_4$ to a matrix $\mat A$. then the column space gets larger unless $\_ \_ \_\_ \_ \_\_ \_ \_\_ \_ \_\_ \_ \_\_ \_ \_$. Give an example where the column space gets larger and an example where it doesn't. 

What should be the condition on $\vec b$ for $\mat A \vec x = \vec b$ to have a solution if the $C(\mat A)$ doesn't get larger?  

\item We begin by making a matrix of all of these vectors. 

\begin{center}

$
\begin{bmatrix}
\vec v_1 & \vec v_2 & \vec v_3 & \vec v_4 & \vec v_5 & \vec v_6  
\end{bmatrix}
$

$
\begin{bmatrix}
1 & 1 & 1 & 0 & 0 & 0 \\
-1 & 0 & 0 & 1 & 1 & 0\\
0 & -1 & 0 & -1 & 0 & 1\\ 
0 & 0 & -1 & 0 & 1 & -1 
\end{bmatrix}
$

\end{center}

We then perform row operations on this matrix:
\begin{center}
$R_1 + R_2$

$R_2 + R_3$

$R_3 + R_4$

$\frac{1}{2} R_4$
\end{center}

The Matrix then becomes 

\begin{center}
$
\begin{bmatrix}
1 & 1 & 1 & 0 & 0 & 0 \\
0 & 1 & 1 & 1 & 1 & 0\\
0 & 0 & 1 & 0 & 1 & 1\\ 
0 & 0 & 0 & 0 & 1 & 0 
\end{bmatrix}
$
\end{center}

Out of these we can see $\vec v_1, \vec v_4, \vec v_6,$ and $\vec v_5$ to be linearly independent.

\textbf{So we have at most 4 linearly independent vectors.}


\item Find the bases for the $C(\mat \cdot)$ and $N(\mat \cdot)$ associated with $\mat A$ and $\mat B$:
\[ A = \left[ \begin{array}{ccc}
1&2&4\\
2&4&8
\end{array} \right]
\hspace{10mm}, B = \left[ \begin{array}{ccc}
1&2&4\\
2&5&8
\end{array} \right]\]

\item If $\mat V$ is the subspace spanned by (1, 1, 1) and (2, 1, 0), find a matrix $\mat A$ that has $\mat V$ as its column space and a matrix $\mat B$ that has $\mat V$ as its null space.

The column space of a matrix is the linear combination of its columns. 



\item Find the complete solution for the following equations and describe the solution space:
\begin{equation} \label{eq1}
\begin{split}
x + 3y + 3z  = 0\\
2x + 6y + 9z = 0\\
-x - 3y +3z  = 0.
\end{split}
\end{equation}
\end{enumerate}

\newpage 

\begin{thebibliography}{}
	\bibitem {LinInd}
		Linear Independence 
		\begin{verbatim}
		 https://textbooks.math.gatech.edu/ila/linear-independence.html
		\end{verbatim}
	\bibitem{dimBases}
		Bases and Dimensions
		\begin{verbatim}
		https://textbooks.math.gatech.edu/ila/dimension.html	
		\end{verbatim}
	\bibitem{specialsolutions}
		Solving Ax = 0: pivot variables, special solutions
		\begin{verbatim}
		https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/
ax-b-and-the-four-subspaces/solving-ax-0-pivot-variables-special-solutions
/MIT18_06SCF11_Ses1.7sum.pdf
		\end{verbatim}

\end{thebibliography}
\end{document}
